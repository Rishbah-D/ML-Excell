{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "Which linear regression training algorithm can you use if you have a\n",
    "training set with millions of features?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stochastic Gradient Desecent` or `Mini Batch GD`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "Suppose the features in your training set have very different scales.\n",
    "Which algorithms might suffer from this, and how? What can you do\n",
    "about it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradient Desecent` algos will be affected as non scaled features will form **_ELONGATED_** curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "Can gradient descent get stuck in a local minimum when training a\n",
    "logistic regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_No GD will algo will get in local minimum during Logistic Regression as the curve of cost function is convex.._**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4\n",
    "\n",
    "Do all gradient descent algorithms lead to the same model, provided you\n",
    "let them run long enough?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_No SGD and Mini Batch will not produce the [SAME Model] as they don't fully converge and keep bouncing around the global minimum_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5\n",
    "\n",
    "Suppose you use batch gradient descent and you plot the validation error\n",
    "at every epoch. If you notice that the validation error consistently goes\n",
    "up, what is likely going on? How can you fix this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Overfitiing` can be the possible reason but **_it maybe that learing rate is too high and model is not able to converge [this case is valid if train error also goes up]_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6\n",
    "\n",
    "Is it a good idea to stop mini-batch gradient descent immediately when\n",
    "the validation error goes up?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`No` as mini-batch GD works with random-ness it is not given that it would make progress at every given step[in short timeframe atleast] so stop immediately may not be the ideal case as it would rob the model of further progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7\n",
    "\n",
    "Which gradient descent algorithm (among those we discussed) will\n",
    "reach the vicinity of the optimal solution the fastest? Which will actually\n",
    "converge? How can you make the others converge as well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Fastest:_** `Stochastic GD` while `Batch GD` is the only one that converges....**_We can reduce the learning rate gradually to make SGD and Mini-Batch GD converge as well_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8\n",
    "\n",
    "Suppose you are using polynomial regression. You plot the learning\n",
    "curves and you notice that there is a large gap between the training error\n",
    "and the validation error. What is happening? What are three ways to\n",
    "solve this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large gap between training and validation error is clear case of `Overfitting`... To resolve this we can 1. Reduce the degree of polynomial.. 2. use regularised models... 3. Increase the size of train set so the model has more instances to learn by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9\n",
    "\n",
    "Suppose you are using ridge regression and you notice that the training\n",
    "error and the validation error are almost equal and fairly high. Would\n",
    "you say that the model suffers from high bias or high variance? Should\n",
    "you increase the regularization hyperparameter Î± or reduce it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`High Bias` and aplha should be decreased [as we learnt in the chap.. if alpha goes up weights tend to zero and flat line forms]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q12\n",
    "\n",
    "Implement batch gradient descent with early stopping for softmax\n",
    "regression without using Scikit-Learn, only NumPy. Use it on a\n",
    "classification task such as the iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
